<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head><meta http-equiv="Content-Type" content="text/html; charset=utf-8">
	<meta name="viewport" content="width=800">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    .hp-photo{ width:240px; height:240px; border-radius:240px; -webkit-border-radius:240px; -moz-border-radius:240px; }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 24px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 15px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
    </style>

    <title>SemanticHD </title>
    <!-- <link rel="icon" type="image/x-icon" href=""> -->
</head>

<body>
<table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
<tbody>
<tr>
<td>

    <!--SECTION 4 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
          <td><heading> Our Research <!--<a href=""></a>--> </heading>
          </td>
          </tr></tbody>
    </table>

    <!--SECTION 5 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody><tr>
			
	<!-- BOPR -->
        <td width="20%"><img src="./imgs/bopr.gif" alt="PontTuset" width="200" style="border-style: none"></td>
            <td width="80%" valign="top">
                 <p><a href="https://semanticdh.github.io/BoPR/">
                 <papertitle>BoPR: Body-aware Part Regressor for Human Shape and Pose Estimation</papertitle></a>
                 <br>
		     <a href="">Yongkang Cheng</a>,
		     <a href="https://scholar.google.com/citations?user=o31BPFsAAAAJ&hl=en">Shaoli Huang</a>,
		     <a href="">Jineng Ning</a>,
	             <a href="https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en&oi=ao">Ying Shan</a>,
		 <br>
                 <em>arXiv preprint.</em>, 2023
                 <br>
                 <a href="https://arxiv.org/abs/2303.11675">arXiv</a> /
                 <a href="https://semanticdh.github.io/BoPR/"><font color="red">Project Page </font></a>/
                 <a href="https://github.com/cyk990422/BoPR"><font color="red">Code</font></a>
                 <iframe src="" frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
                 <p align="justify" style="font-size:13px"> This paper presents a novel approach for estimating human body shape and pose from monocular images that effectively addresses the challenges of occlusions and depth ambiguity.
                 </p>
                <p></p>
            </td>
        </tr>
			
	<!-- HMC -->
        <td width="20%"><img src="./imgs/HMC.jpg" alt="PontTuset" width="200" style="border-style: none"></td>
            <td width="80%" valign="top">
                 <p><a href="https://semanticdh.github.io/HMC/">
                 <papertitle>HMC: Hierarchical Mesh Coarsening for Skeleton-free Motion Retargeting</papertitle></a>
                 <br>
		     <a href="https://orcid.org/0000-0002-3357-0080">Haoyu Wang</a>,
		     <a href="https://scholar.google.com/citations?user=o31BPFsAAAAJ&hl=en">Shaoli Huang</a>,
		     <a href="https://scholar.google.com/citations?hl=zh-CN&user=4C7mvOwAAAAJ">Fang Zhao</a>,
		     <a href="https://scholar.google.com/citations?hl=zh-CN&user=fYdxi2sAAAAJ">Chun Yuan</a>,
	             <a href="https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en&oi=ao">Ying Shan</a>,
		 <br>
                 <em>arXiv preprint.</em>, 2023
                 <br>
                 <a href="https://arxiv.org/abs/2303.10941">arXiv</a> /
                 <a href=""><font color="red">Project Page </font></a>/
                 <a href=""><font color="red">Code</font></a>
                 <iframe src="" frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
                 <p align="justify" style="font-size:13px">HMC shows a simple yet effective way of better handling local-part motions in the skeleton-free motion retaregting task, and it also fixes many failure cases in previous literature regarding small-part motions and local-motion interdependencies.
                 </p>
                <p></p>
            </td>
        </tr>

	<!-- ACR -->
        <td width="20%"><img src="./imgs/p1.GIF" alt="PontTuset" width="200" style="border-style: none"></td>
            <td width="80%" valign="top">
	             <p><a href="https://semanticdh.github.io/ACR/">
	             <papertitle>ACR: Attention Collaboration-based Regressor for Arbitrary Two-Hand Reconstruction</papertitle></a>
                     <br>
		     <a href="https://github.com/ZhengdiYu">Zhengdi Yu</a>,
		     <a href="https://scholar.google.com/citations?user=o31BPFsAAAAJ&hl=en">Shaoli Huang*</a>,
		     <a href="http://fangchen.org/">Chen Fang</a>,
		     <a href="https://breckon.org/toby/research/">Toby P. Breckon</a>,
		     <a href="https://juewang725.github.io/">Jue Wang</a>
		     <br>
                 <em>Conference on Computer Vision and Pattern Recognition (CVPR) </em>, 2023
                 <br>
                 <!--<font color="red"><strong>..</strong></font><br>-->
                 <a href="https://arxiv.org/abs/2303.05938">arXiv</a> /
                 <a href="https://semanticdh.github.io/ACR/"><font color="red">Project Page </font></a>/
                 <a href="https://github.com/ZhengdiYu/Arbitrary-Hands-3D-Reconstruction"><font color="red">Code</font></a>
                 <iframe src="https://ghbtns.com/github-btn.html?user=ZhengdiYu&repo=Arbitrary-Hands-3D-Reconstruction&type=star&count=true&size=small"
                 frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
			     <p align="justify" style="font-size:13px">We introduce the first one-stage arbitrary hand reconstruction method using only a monocular RGB image as input.</p>
                <p></p>
            </td>
        </tr>
	
	<!-- anchordef -->
        <td width="20%"><img src="./imgs/anchordef.gif" alt="PontTuset" width="200" style="border-style: none"></td>
            <td width="80%" valign="top">
	             <p><a href="https://semanticdh.github.io/AnchorDEF/">
	             <papertitle>Learning Anchor Transformations for 3D Garment Animation</papertitle></a>
                     <br>
		     <a href="https://scholar.google.com/citations?hl=zh-CN&user=4C7mvOwAAAAJ">Fang Zhao</a>,
             <a href="">Zekun Li</a>,
		     <a href="https://scholar.google.com/citations?user=o31BPFsAAAAJ&hl=en">Shaoli Huang*</a>,
		     <a href="">Junwu Weng</a>,
		     <a href="">Tianfei Zhou</a>,
		     <a href="">Guo-Sen Xie</a>,
             <a href="">Jue Wang</a>,
             <a href="">Ying Shan</a>
		     <br>
                 <em>Conference on Computer Vision and Pattern Recognition (CVPR) </em>, 2023
                 <br>
                 <!--<font color="red"><strong>..</strong></font><br>-->
                 <a href="https://arxiv.org/abs/2304.00761">arXiv</a> /
                 <a href="https://semanticdh.github.io/AnchorDEF/"><font color="red">Project Page </font></a>/
                 <a href=""><font color="red">Code</font></a>
                 <iframe src="" frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
			     <p align="justify" style="font-size:13px">This paper proposes an anchor-based deformation model, namely AnchorDEF, to predict 3D garment animation from a body motion sequence.</p>
                <p></p>
            </td>
        </tr>


	<!-- R2ET -->
        <td width="20%"><img src="./imgs/R2ET.gif" alt="PontTuset" width="200" style="border-style: none"></td>
            <td width="80%" valign="top">
	          <p><a href="https://semanticdh.github.io/R2ET/">
	          <papertitle>Skinned Motion Retargeting with Residual Perception of Motion Semantics & Geometry</papertitle></a>
                  <br>
		     <a href="https://kebii.github.io/">Jiaxu Zhang</a>,
		     <a href="https://scholar.google.com/citations?user=kujuGQoAAAAJ&hl=en">Junwu Weng</a>,
		     Di Kang,
		     <a href="https://scholar.google.com/citations?hl=zh-CN&user=4C7mvOwAAAAJ">Fang Zhao</a>,
		     <a href="https://scholar.google.com/citations?user=o31BPFsAAAAJ&hl=en">Shaoli Huang</a>,
	             Xuefei Zhe,
	             Linchao Bao,
	             <a href="https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en&oi=ao">Ying Shan</a>,
		     <a href="https://juewang725.github.io/">Jue Wang</a>,
		     Zhigang Tu*
		   <br>
		   <em>Conference on Computer Vision and Pattern Recognition (CVPR) </em>, 2023
                   <br>
                   <!--<font color="red"><strong>..</strong></font><br>-->
                   <a href="https://semanticdh.github.io/R2ET/static/pdfs/CVPR_2023_R2ET_camera_ready.pdf">arXiv</a> /
                   <a href="https://semanticdh.github.io/R2ET/"><font color="red">Project Page </font></a>/
                   <a href="https://github.com/Kebii/R2ET"><font color="red">Code</font></a>
                   <iframe src="https://ghbtns.com/github-btn.html?user=Kebii&repo=R2E&type=star&count=true&size=small"
                   frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
			     <p align="justify" style="font-size:13px">R2ET is a neural motion retargeting model that can preserve source motion semantics and avoid interpenetration in target motion.</p>
                <p></p>
            </td>
        </tr>




        <!--
        <td width="20%"><img src="./imgs/19_neurips_3d_bonet.gif" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	             <p><a href="https://arxiv.org/abs/1906.01140">
	             <papertitle>Learning Object Bounding Boxes for 3D Instance Segmentation on Point Clouds</papertitle></a>
                 <br><strong>B. Yang</strong>, J. Wang, R. Clark, Q. Hu, S. Wang, A. Markham, N. Trigoni
                 <br>
                 <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2019 <font color="red"><strong>(Spotlight, 200/6743)</strong></font>
                 <br>
                 <a href="https://arxiv.org/abs/1906.01140">arXiv</a> /
                 <a href="http://kaldir.vc.in.tum.de/scannet_benchmark/result_details?id=118">ScanNet Benchmark</a> /
                 <a href="https://www.reddit.com/r/MachineLearning/comments/bx8jhz/r_new_sota_for_3d_object_detection/">Reddit Discussion</a> /
                 <font color="red"> News:</font>
                 <a href="https://mp.weixin.qq.com/s/jHbWf_SSZE_J6NRJR-96sQ"><font color="red">(新智元,</font></a>
                 <a href="https://mp.weixin.qq.com/s/4GPkmTri4Vk7Xy0J8TiBNw"><font color="red">图像算法,</font></a>
                 <a href="https://mp.weixin.qq.com/s/C1FDPkAkmnmAZ_gvvtzBHw"><font color="red">AI科技评论,</font></a>
                 <a href="https://mp.weixin.qq.com/s/wViZITtsb4j3oFtOpJI9wQ"><font color="red">将门创投,</font></a>
                 <a href="https://mp.weixin.qq.com/s/S7mHrOxOwTIhDGPhu1SI4A"><font color="red">CVer,</font></a>
                 <a href="https://mp.weixin.qq.com/s/gybhVw3D4ykAHsVGzazWNw"><font color="red">泡泡机器人)</font>/</a>
                 <a href="https://www.youtube.com/watch?v=Bk727Ec10Ao"><font color="red">Video</font></a>/
                 <a href="https://github.com/Yang7879/3D-BoNet"><font color="red">Code</font></a>
                 <iframe src="https://ghbtns.com/github-btn.html?user=Yang7879&repo=3D-BoNet&type=star&count=true&size=small"
                 frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
			     <p align="justify" style="font-size:13px">We propose a simple and efficient neural architecture for accurate 3D instance segmentation on point clouds.
                  It achieves the SOTA performance on ScanNet and S3DIS (June 2019).</p>
                <p></p>
            </td>
        </tr>
	-->

	
        </tbody>
    </table>

    <!--SECTION 7 -->
<!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td><heading>Members <a href=""> (Full list at ...)</a> </heading>
              <p> <strong><a href="https://scholar.google.com/citations?user=o31BPFsAAAAJ&hl=en&oi=ao">Shaoli Huang</a></strong>: &ensp;&ensp; Visual Computing Center, Tencent AI Lab.</p>
          </td>
       </tr></tbody>
    </table>
//-->
    <!--SECTION 9 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr>
    <td width="100%" align="middle">
    <p align="center" style="width: 33% ">
    <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=FAcW1eE0mLMSgDewwZS9lllfyB0DKCYtyN0uTXn1ZHY"></script>
    </p></td>
    </tr>
    </tbody>
    </table>


    <!--SECTION 10 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
         <tbody><tr>
            <td><br>
               <!--<p align="right"><font size="3">Erd&ouml;s = ? </font><br> -->
		       <p align="right"><font size="2"> Last update: 2023.03.20 <!--<a href="http://www.cs.berkeley.edu/~barron/">Thanks.</a></font></p>-->
            </td>
         </tr>
         </tbody>
     </table>


</td>
</tr>
</tbody>
</table>
</body>
</html>
